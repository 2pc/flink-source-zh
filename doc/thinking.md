前面已经写过了太多的Flink源码分析的东西，可能讲得比较偏理论，今天呢，就来分享一个实际工作中碰到的Flink场景下的问题，也是自己编码过程中的一个教训。
先来简单描述一下场景，实际的场景比较复杂，这里就简单的抽象一下。

在电商业务中，如果我们需要对每一笔订单进行统计来计算销售额，比如说，618和双十一时的订单金额统计。对于每一个用户来说，他可能先在电商网站上下一个单，
这个订单会包含多个所要购买的商品，此时就需要将其金额加入到实时销售额里面，而我们同时可以对这个订单进行不同的操作，比如修改物流地址、取消订单等。那么，
在电商大促实时大屏上就应该根据我们对其的实际操作增减销售额，这就需要保证对每个订单的顺序才能保证处理的准确性，如果取消订单的消息的处理早于下单和修改
物流地址，那么最终的处理结果就可能不正确。那么如何保证实时数据处理的可靠性？

假设我们使用的是kafka接入订单流，然后进行处理，就变成了三个问题：如何保证kafka接入数据的顺序、如何保证flink处理的顺序、如何保证数据输出的顺序？

首先，如何保证kafka接入数据的顺序？emmmmmm....这是个在实时计算面试中经常会问的问题，但是请一定注意，这是个陷阱问题，因为kafka根本就无法保证全局
消息的有序性，在它的各个分区间的消息一定是无法保证顺序的，但是kafka各个分区内的数据是有序的，那么我们就可以利用上这个特性。如果我们在使用kafka接
入数据时，将相同订单放到同一分区，那么就能保证订单的创建消息一定早于订单的取消的消息，即第一个问题解决。

