### 数据去重

数据去重是大数据领域经常碰见的问题，其目的在于清洗掉不可靠数据源产生的脏数据，使得计算结果更加精确。在常见的实时数据去重方案中，
HashSet由于是实现简单使用方便而得到了广泛的应用。此外，也还有一些其它的方案，如：布隆过滤器(Bloom Filter)、RocksDB StateBackend
以及外部存储。下面分别就这三种去重方案进行分析：
  * 布隆过滤器(Bloom Filter)：作为消耗较少效率较高的方案在允许一定的误判的情况下是首选，如果不考虑自己重复造一遍轮子，可以
  使用Guava的BloomFilter来实现去重，使用put()方法放入数据，使用mightContain()判断是否存在，其特点是如果它判断不存在则数
  据一定不存在，如果它判断存在，则数据有一定的概率不存在(这叫做假阳性概率，在构建BloomFilter时可以指定)。
  * RocksDB StateBackend：前面已经提到布隆过滤器存在一定概率的误判，因此在一些要求非常精确的场合并不适用。在必须保证非常准确
  的场景下，可以选择使用Flink内置的RocksDB状态后端，后果是状态会极其巨大。
  * 引入外部K-V存储：实际上，这种方案与上述的RocksDB状态后端方案相差不大，因为RocksDB也是一种K-V存储，只不过它是由Flink所管
  理的，如果不想在Flink中维护巨大的状态，就可以选择此种方案。缺点是如果作业重启，外部存储是不会同步恢复到一致的状态的，此时结果可
  能出现偏差。